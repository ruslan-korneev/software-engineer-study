# CAP теорема

## Введение

CAP теорема — одна из фундаментальных концепций в проектировании распределённых систем. Она описывает компромиссы, которые неизбежно приходится делать при создании систем, работающих на нескольких узлах.

---

## 1. История и автор теоремы

### Eric Brewer и рождение теоремы

CAP теорема была впервые представлена **Эриком Брюером (Eric Brewer)** в 2000 году на конференции **ACM Symposium on Principles of Distributed Computing (PODC)**. В то время Брюер был профессором Калифорнийского университета в Беркли и сооснователем компании Inktomi.

Изначально это была **гипотеза**, основанная на практическом опыте построения распределённых систем. Брюер наблюдал, что при проектировании крупномасштабных интернет-сервисов приходилось выбирать между различными гарантиями.

### Формальное доказательство

В 2002 году **Сет Гилберт (Seth Gilbert)** и **Нэнси Линч (Nancy Lynch)** из MIT опубликовали формальное доказательство теоремы в статье "Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services". С этого момента гипотеза стала теоремой.

### Контекст появления

В начале 2000-х интернет-компании (Google, Amazon, Yahoo) столкнулись с необходимостью обрабатывать огромные объёмы данных. Традиционные реляционные СУБД не справлялись с нагрузкой, и началась эра распределённых систем хранения данных. CAP теорема помогла систематизировать понимание компромиссов в таких системах.

---

## 2. Три свойства CAP

CAP — это акроним, обозначающий три фундаментальных свойства распределённых систем:

```
┌─────────────────────────────────────────────────────────┐
│                     CAP Theorem                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│                    Consistency                          │
│                        /\                               │
│                       /  \                              │
│                      /    \                             │
│                     / CP   \  CA                        │
│                    /   ★    \  ★                        │
│                   /          \                          │
│                  /     AP     \                         │
│                 /      ★       \                        │
│                /________________\                       │
│         Availability          Partition                 │
│                               Tolerance                 │
│                                                         │
│  ★ CP - Consistency + Partition Tolerance              │
│  ★ AP - Availability + Partition Tolerance             │
│  ★ CA - Consistency + Availability (только теория)     │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Краткое определение

| Свойство | Описание |
|----------|----------|
| **C** (Consistency) | Все узлы видят одинаковые данные в один момент времени |
| **A** (Availability) | Каждый запрос получает ответ (успех или ошибка) |
| **P** (Partition Tolerance) | Система продолжает работать при сетевых разделениях |

---

## 3. Подробное объяснение каждого свойства

### 3.1 Consistency (Согласованность)

**Определение**: Все узлы распределённой системы видят одни и те же данные в одно и то же время. После успешной записи все последующие чтения вернут это новое значение.

```
Линеаризуемость (Linearizability) — строгая форма согласованности:

Клиент A: ──write(x=1)──┐
                        │
                        ▼
        ┌───────────────────────────────┐
        │      Все узлы видят x=1       │
        └───────────────────────────────┘
                        │
                        ▼
Клиент B: ──────────────read(x)── возвращает 1
```

#### Пример согласованной системы

```python
# Сценарий: банковский перевод
# Начальное состояние: account_balance = 1000

# Клиент A выполняет операцию
def transfer_money():
    current = read_balance()  # 1000
    new_balance = current - 500
    write_balance(new_balance)  # 500

# В согласованной системе:
# Клиент B сразу после этого видит balance = 500
# Независимо от того, какой узел обрабатывает запрос
```

#### Типы согласованности

```
┌──────────────────────────────────────────────────────────┐
│              Спектр моделей согласованности               │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  Строгая ◄────────────────────────────────────► Слабая   │
│                                                          │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐ │
│  │Lineariz- │  │Sequential│  │ Causal   │  │ Eventual │ │
│  │ability   │  │Consistency│ │Consistency│ │Consistency│ │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘ │
│       ▲              ▲             ▲             ▲       │
│       │              │             │             │       │
│   Все видят      Операции      Причинно-    Данные      │
│   одно и то же   упорядочены   связанные   сойдутся     │
│   в реальном     для каждого   операции    "когда-      │
│   времени        клиента       упорядочены  нибудь"     │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

**Важно**: В контексте CAP теоремы под Consistency понимается **линеаризуемость (linearizability)** — самая строгая форма согласованности.

---

### 3.2 Availability (Доступность)

**Определение**: Каждый запрос к работающему узлу системы получает ответ — либо успешный, либо с ошибкой, но без бесконечного ожидания.

```
Высокая доступность:

Клиент ──запрос──► [Узел 1] ──ответ──► Клиент ✓
                      │
                      ▼
               (время < timeout)

Даже если другие узлы недоступны, работающий узел
ДОЛЖЕН ответить на запрос
```

#### Количественное измерение доступности

```
┌─────────────────────────────────────────────────────────┐
│              Уровни доступности (SLA)                   │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Доступность     Простой в год    Пример               │
│  ────────────    ─────────────    ───────              │
│  99%             3.65 дней        Типичный сайт        │
│  99.9%           8.76 часов       Бизнес-приложения    │
│  99.99%          52.56 минут      Банковские системы   │
│  99.999%         5.26 минут       Критические системы  │
│  99.9999%        31.5 секунд      Телеком, медицина    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

#### Пример: доступность vs согласованность

```python
# Сценарий: интернет-магазин, остаток товара на складе

# Высокая доступность (AP-система):
def check_availability(product_id):
    # Отвечаем сразу, даже если данные могут быть устаревшими
    local_stock = local_cache.get(product_id)
    return {"in_stock": local_stock > 0}
    # Риск: можем продать товар, которого уже нет

# Высокая согласованность (CP-система):
def check_availability(product_id):
    # Ждём подтверждения от всех узлов
    try:
        stock = distributed_read(product_id, quorum=ALL)
        return {"in_stock": stock > 0}
    except TimeoutError:
        # Не отвечаем, если не можем гарантировать точность
        raise ServiceUnavailable()
```

---

### 3.3 Partition Tolerance (Устойчивость к разделению)

**Определение**: Система продолжает функционировать, несмотря на потерю или задержку сообщений между узлами сети.

```
Network Partition (Сетевое разделение):

┌─────────────────┐        XXXXX        ┌─────────────────┐
│    Partition A  │       XX   XX       │    Partition B  │
│                 │      X       X      │                 │
│   ┌─────────┐   │     X  Сеть   X     │   ┌─────────┐   │
│   │  Узел 1 │───┼────X разорвана X────┼───│  Узел 3 │   │
│   └─────────┘   │     X         X     │   └─────────┘   │
│        │        │      X       X      │        │        │
│   ┌─────────┐   │       XX   XX       │   ┌─────────┐   │
│   │  Узел 2 │   │        XXXXX        │   │  Узел 4 │   │
│   └─────────┘   │                     │   └─────────┘   │
│                 │                     │                 │
└─────────────────┘                     └─────────────────┘

Узлы 1 и 2 могут общаться между собой
Узлы 3 и 4 могут общаться между собой
Но группы НЕ могут общаться друг с другом
```

#### Причины сетевых разделений

```
┌─────────────────────────────────────────────────────────┐
│         Причины Network Partition                       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Физические проблемы                                 │
│     - Обрыв кабеля                                      │
│     - Сбой сетевого оборудования (роутер, свитч)       │
│     - Проблемы у провайдера                            │
│                                                         │
│  2. Программные проблемы                                │
│     - Неправильная конфигурация firewall               │
│     - Исчерпание ресурсов (CPU, память, сокеты)        │
│     - Баги в сетевом стеке                              │
│                                                         │
│  3. Высокая нагрузка                                    │
│     - Перегрузка сети (congestion)                      │
│     - Слишком много соединений                          │
│     - DDoS-атаки                                        │
│                                                         │
│  4. Географическое распределение                        │
│     - Проблемы с подводными кабелями                   │
│     - Политические ограничения                          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

#### Почему P обязателен в распределённых системах

```python
# В реальном мире сетевые разделения НЕИЗБЕЖНЫ

# Исследование от Google (Bailis & Kingsbury, 2014):
# - Разделения в датацентрах случаются регулярно
# - Средняя длительность: от секунд до нескольких часов

# Примеры реальных инцидентов:
incidents = [
    "Amazon S3 outage (2017) - 4+ часа",
    "GitHub network partition (2020) - несколько часов",
    "Cloudflare outage (2020) - 27 минут",
]

# Вывод: если ваша система распределённая,
# вы ДОЛЖНЫ быть готовы к partition
# Это не "если", а "когда"
```

---

## 4. Почему можно выбрать только 2 из 3

### Теоретическое обоснование

Суть CAP теоремы: в момент сетевого разделения (Partition) вы **вынуждены** выбирать между Consistency и Availability.

```
Сценарий: сетевое разделение произошло

┌──────────────┐                    ┌──────────────┐
│   Узел A     │    X  Разрыв  X    │   Узел B     │
│   data = v1  │    X ──────── X    │   data = v1  │
└──────────────┘                    └──────────────┘
       │                                   │
       ▼                                   ▼
  Клиент пишет                       Клиент читает
    data = v2                          data = ?
       │                                   │
       ▼                                   ▼
  ┌────────────┐                    ┌────────────┐
  │ data = v2  │  НЕ МОЖЕТ передать │ data = v1  │
  │            │  ────────────────► │            │
  └────────────┘                    └────────────┘

Вопрос: что вернуть клиенту на узле B?
```

### Два возможных выбора

```
┌─────────────────────────────────────────────────────────┐
│               ВЫБОР 1: Приоритет Consistency (CP)       │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Узел B: "Я не могу связаться с Узлом A,               │
│           не знаю актуальное значение.                  │
│           Откажу в обслуживании."                       │
│                                                         │
│  → Клиент получает ошибку (нет Availability)           │
│  → Но данные остаются согласованными                    │
│                                                         │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│               ВЫБОР 2: Приоритет Availability (AP)      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Узел B: "Отвечу тем, что знаю: data = v1"              │
│                                                         │
│  → Клиент получает ответ (есть Availability)           │
│  → Но данные устаревшие (нет Consistency)              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Математическое доказательство (упрощённо)

```
Допустим, мы хотим и C, и A при partition (P):

1. Partition случился: узлы A и B изолированы
2. Клиент пишет x=2 на узел A
3. Узел A не может отправить обновление на B (partition)
4. Клиент читает x с узла B

Если гарантируем A (доступность):
   - B должен ответить
   - B не знает о x=2
   - B отвечает x=1 (старое значение)
   - Нарушена C (согласованность) ❌

Если гарантируем C (согласованность):
   - B должен вернуть x=2
   - B не знает о x=2
   - B НЕ МОЖЕТ ответить корректно
   - B должен отказать в ответе
   - Нарушена A (доступность) ❌

Невозможно одновременно гарантировать C и A при P.

Q.E.D. ∎
```

---

## 5. CP системы (Consistency + Partition Tolerance)

### Характеристики CP-систем

```
┌─────────────────────────────────────────────────────────┐
│                    CP-системы                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Приоритет: Согласованность данных                     │
│  Жертва: Доступность во время partition                │
│                                                         │
│  Поведение при partition:                               │
│  ┌─────────────────────────────────────────────────┐   │
│  │  • Часть узлов становится недоступной           │   │
│  │  • Запросы к minority partition отклоняются     │   │
│  │  • Только majority partition обслуживает запросы│   │
│  │  • Возможны таймауты и ошибки                   │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
│  Применение:                                            │
│  • Финансовые транзакции                               │
│  • Системы бронирования                                │
│  • Инвентаризация товаров                              │
│  • Любые данные, где ошибка дороже недоступности       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Примеры CP-систем

#### MongoDB (с WriteConcern: majority)

```javascript
// MongoDB как CP-система (при правильной настройке)

// Настройка write concern для согласованности
db.collection.insertOne(
    { item: "phone", price: 999 },
    {
        writeConcern: {
            w: "majority",  // Запись подтверждена большинством
            j: true,        // Записано в journal
            wtimeout: 5000  // Таймаут 5 секунд
        }
    }
);

// При partition:
// - Minority partition НЕ МОЖЕТ принимать записи
// - Majority partition продолжает работать
// - Клиенты minority получают ошибки
```

```
MongoDB Replica Set при partition:

До partition:
┌─────────────────────────────────────────────────────────┐
│   Primary ◄─────► Secondary 1 ◄─────► Secondary 2      │
│     [1]              [2]                [3]             │
│            Все узлы синхронизированы                    │
└─────────────────────────────────────────────────────────┘

После partition:
┌────────────────┐        ┌────────────────────────────┐
│   Minority     │  XXX   │      Majority              │
│                │  XXX   │                            │
│  Secondary 2   │  XXX   │  Primary ◄──► Secondary 1 │
│     [3]        │  XXX   │    [1]          [2]        │
│                │        │                            │
│ READ-ONLY или  │        │  Продолжает обслуживать   │
│ недоступен     │        │  чтение И запись          │
└────────────────┘        └────────────────────────────┘
```

#### HBase

```java
// HBase — CP-система, построенная на HDFS и ZooKeeper

// Пример: чтение с гарантией согласованности
Configuration config = HBaseConfiguration.create();
Connection connection = ConnectionFactory.createConnection(config);
Table table = connection.getTable(TableName.valueOf("users"));

// Get всегда читает актуальные данные
Get get = new Get(Bytes.toBytes("user123"));
Result result = table.get(get);

// При partition:
// - RegionServer недоступен → регион недоступен
// - ZooKeeper определяет "живые" узлы
// - Failover занимает время (десятки секунд)
// - В это время часть данных недоступна
```

#### Redis Cluster (в режиме с согласованностью)

```python
# Redis Cluster — можно настроить как CP

import redis

# Создание кластера с требованием кворума
cluster = redis.RedisCluster(
    host='redis-cluster.example.com',
    port=6379,
    # При потере связи с master — ошибка
    skip_full_coverage_check=False
)

# WAIT команда для синхронной репликации
def write_with_consistency(key, value):
    pipe = cluster.pipeline()
    pipe.set(key, value)
    # Ждём подтверждения от 2 реплик в течение 1 секунды
    pipe.wait(2, 1000)
    result = pipe.execute()

    if result[1] < 2:
        raise ConsistencyError("Не удалось реплицировать на нужное число узлов")
```

#### etcd и Consul

```go
// etcd — типичная CP-система (использует Raft)
// Используется Kubernetes для хранения состояния

import (
    "context"
    clientv3 "go.etcd.io/etcd/client/v3"
)

func writeToEtcd(key, value string) error {
    cli, _ := clientv3.New(clientv3.Config{
        Endpoints: []string{"etcd1:2379", "etcd2:2379", "etcd3:2379"},
    })

    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    // Put гарантирует запись в majority
    // При partition minority — операция завершится ошибкой
    _, err := cli.Put(ctx, key, value)
    return err
}
```

```
etcd Raft Consensus при partition:

Нормальная работа (5 узлов, нужно 3 для кворума):
┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
│ N1  │ │ N2  │ │ N3  │ │ N4  │ │ N5  │
│Lead │◄│Foll │◄│Foll │◄│Foll │◄│Foll │
└─────┘ └─────┘ └─────┘ └─────┘ └─────┘
     ▲────────────────────────────┘
              Репликация

Partition (2 + 3 узла):
┌──────────────┐     ┌──────────────────────┐
│   Minority   │ XXX │      Majority        │
│  ┌─────┐     │ XXX │  ┌─────┐ ┌─────┐    │
│  │ N1  │     │ XXX │  │ N3  │ │ N4  │    │
│  │Lead │     │ XXX │  │Foll │ │Foll │    │
│  └─────┘     │     │  └─────┘ └─────┘    │
│  ┌─────┐     │     │  ┌─────┐            │
│  │ N2  │     │     │  │ N5  │ ← New Lead │
│  │Foll │     │     │  │     │            │
│  └─────┘     │     │  └─────┘            │
│              │     │                     │
│ НЕ МОЖЕТ     │     │ Выбирают нового     │
│ принимать    │     │ лидера, продолжают  │
│ записи       │     │ работу              │
└──────────────┘     └──────────────────────┘
```

---

## 6. AP системы (Availability + Partition Tolerance)

### Характеристики AP-систем

```
┌─────────────────────────────────────────────────────────┐
│                    AP-системы                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Приоритет: Доступность системы                        │
│  Жертва: Строгая согласованность                       │
│                                                         │
│  Поведение при partition:                               │
│  ┌─────────────────────────────────────────────────┐   │
│  │  • ВСЕ узлы продолжают отвечать на запросы      │   │
│  │  • Каждый узел работает со своей версией данных│   │
│  │  • После восстановления — слияние (merge)       │   │
│  │  • Возможны конфликты, требующие разрешения     │   │
│  └─────────────────────────────────────────────────┘   │
│                                                         │
│  Применение:                                            │
│  • Социальные сети (лайки, посты)                      │
│  • DNS-системы                                          │
│  • Кэширование                                          │
│  • Данные, где временная рассогласованность OK         │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Примеры AP-систем

#### Apache Cassandra

```python
# Cassandra — классическая AP-система

from cassandra.cluster import Cluster
from cassandra.policies import DCAwareRoundRobinPolicy
from cassandra import ConsistencyLevel

cluster = Cluster(
    ['node1', 'node2', 'node3'],
    load_balancing_policy=DCAwareRoundRobinPolicy()
)
session = cluster.connect('my_keyspace')

# Запись с низким consistency level — максимальная доступность
session.execute(
    "INSERT INTO users (id, name, email) VALUES (%s, %s, %s)",
    (user_id, name, email),
    consistency_level=ConsistencyLevel.ONE  # Достаточно одного узла
)

# Чтение также с ONE — может вернуть устаревшие данные
session.execute(
    "SELECT * FROM users WHERE id = %s",
    (user_id,),
    consistency_level=ConsistencyLevel.ONE
)
```

```
Cassandra при partition (RF=3):

Нормальная работа:
┌─────────────────────────────────────────────────────────┐
│  Data: user_123                                         │
│                                                         │
│  ┌──────┐    ┌──────┐    ┌──────┐    ┌──────┐         │
│  │Node 1│◄──►│Node 2│◄──►│Node 3│◄──►│Node 4│         │
│  │ [R1] │    │ [R2] │    │ [R3] │    │      │         │
│  └──────┘    └──────┘    └──────┘    └──────┘         │
│                                                         │
│  R1, R2, R3 — реплики данных                           │
└─────────────────────────────────────────────────────────┘

При partition:
┌────────────────────┐     ┌────────────────────────────┐
│    Partition A     │ XXX │       Partition B          │
│  ┌──────┐          │ XXX │  ┌──────┐    ┌──────┐     │
│  │Node 1│ [R1]     │ XXX │  │Node 3│    │Node 4│     │
│  │      │          │     │  │ [R3] │    │      │     │
│  └──────┘          │     │  └──────┘    └──────┘     │
│  ┌──────┐          │     │                           │
│  │Node 2│ [R2]     │     │  Могут принимать записи   │
│  │      │          │     │  на свою реплику [R3]     │
│  └──────┘          │     │                           │
│                    │     │                           │
│  Могут принимать   │     │                           │
│  записи на [R1,R2] │     │                           │
└────────────────────┘     └────────────────────────────┘

После восстановления: Anti-entropy repair сливает данные
```

#### Amazon DynamoDB

```python
# DynamoDB — AP-система от Amazon

import boto3

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('Products')

# Eventually consistent read (по умолчанию) — AP поведение
response = table.get_item(
    Key={'product_id': '123'},
    ConsistentRead=False  # Eventually consistent
)

# Можно запросить строгую согласованность (удар по доступности)
response = table.get_item(
    Key={'product_id': '123'},
    ConsistentRead=True   # Strongly consistent — больше латентность
)

# Conditional writes для разрешения конфликтов
table.put_item(
    Item={
        'product_id': '123',
        'stock': 50,
        'version': 2
    },
    ConditionExpression='version = :v',
    ExpressionAttributeValues={':v': 1}
)
```

#### CouchDB

```javascript
// CouchDB — AP-система с multi-master репликацией

// Документ может быть изменён на любом узле
// При конфликтах сохраняются ОБЕ версии

// Пример конфликта:
// Node A: {"_id": "doc1", "name": "Alice", "_rev": "2-abc"}
// Node B: {"_id": "doc1", "name": "Alicia", "_rev": "2-xyz"}

// CouchDB выбирает "победителя" детерминированно
// Но сохраняет конфликтующие версии для ручного разрешения

db.get('doc1', {conflicts: true}, function(err, doc) {
    if (doc._conflicts) {
        // Есть конфликтующие версии
        doc._conflicts.forEach(function(rev) {
            // Получаем конфликтующую версию
            db.get('doc1', {rev: rev}, function(err, conflictDoc) {
                // Разрешаем конфликт (например, merge)
                resolveConflict(doc, conflictDoc);
            });
        });
    }
});
```

#### Riak

```erlang
%% Riak — AP-система с vector clocks для отслеживания версий

%% Настройка bucket с разрешением конфликтов
{ok, Bucket} = riakc_pb_socket:get_bucket(Pid, <<"users">>),
NewBucket = riakc_bucket:set_properties([
    {allow_mult, true},      %% Разрешить множественные версии
    {last_write_wins, false} %% НЕ использовать простой LWW
], Bucket),
riakc_pb_socket:set_bucket(Pid, <<"users">>, NewBucket).

%% При чтении можем получить siblings (конфликтующие версии)
%% Приложение должно разрешить конфликт
```

```
Разрешение конфликтов в AP-системах:

┌─────────────────────────────────────────────────────────┐
│         Стратегии разрешения конфликтов                │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Last Write Wins (LWW)                              │
│     - Побеждает запись с большим timestamp             │
│     - Просто, но может терять данные                    │
│     - DynamoDB, Cassandra (опционально)                │
│                                                         │
│  2. Vector Clocks                                       │
│     - Отслеживание причинности изменений               │
│     - Можно определить, есть ли конфликт               │
│     - Riak, Voldemort                                  │
│                                                         │
│  3. CRDTs (Conflict-free Replicated Data Types)        │
│     - Структуры данных, которые можно безопасно        │
│       объединять                                        │
│     - Counters, Sets, Maps                              │
│     - Riak, Redis (CRDT режим)                         │
│                                                         │
│  4. Application-level resolution                        │
│     - Приложение само решает конфликты                 │
│     - Максимальная гибкость                             │
│     - CouchDB                                          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 7. CA системы — почему они практически невозможны

### Теоретическая CA-система

```
CA-система (Consistency + Availability без Partition Tolerance):

┌─────────────────────────────────────────────────────────┐
│                                                         │
│    "Система гарантирует согласованность и              │
│     доступность, но НЕ переживёт сетевое разделение"   │
│                                                         │
│    Это означает: при partition система                 │
│    ПОЛНОСТЬЮ прекращает работу                          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Почему CA невозможна в распределённых системах

```
┌─────────────────────────────────────────────────────────┐
│    Проблема: Partition НЕИЗБЕЖЕН в распределённой      │
│    системе                                              │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Физические ограничения сети                        │
│     - Скорость света конечна                           │
│     - Сети ненадёжны по определению                    │
│     - Аппаратные сбои случаются                         │
│                                                         │
│  2. Практические наблюдения                             │
│     - Google: сетевые проблемы — ежедневная норма      │
│     - Amazon: datacenter isolation happens             │
│     - Любой, кто работал с сетями: "сеть — это ложь"  │
│                                                         │
│  3. Логическое противоречие                            │
│     - "Распределённая" = несколько узлов              │
│     - Несколько узлов = связь по сети                 │
│     - Сеть = возможность partition                     │
│     - Нельзя быть распределённым без риска partition  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Что называют CA-системами (некорректно)

```
"CA-системы" на практике:

┌─────────────────────────────────────────────────────────┐
│                                                         │
│  Традиционные RDBMS (PostgreSQL, MySQL на одном узле)  │
│                                                         │
│  ┌──────────────────────────────────────────────────┐  │
│  │                                                   │  │
│  │   ┌─────────────────┐                            │  │
│  │   │   Один сервер   │                            │  │
│  │   │   PostgreSQL    │                            │  │
│  │   │                 │                            │  │
│  │   │  C: ✓ ACID      │                            │  │
│  │   │  A: ✓ (пока жив)│                            │  │
│  │   │  P: ✗ один узел │                            │  │
│  │   │                 │                            │  │
│  │   └─────────────────┘                            │  │
│  │                                                   │  │
│  │   Это НЕ распределённая система!                 │  │
│  │   CAP к ней не применима в полном смысле        │  │
│  │                                                   │  │
│  └──────────────────────────────────────────────────┘  │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Master-Slave репликация — это не CA

```
Часто ошибочно называют CA:

PostgreSQL с синхронной репликацией:

┌──────────────┐              ┌──────────────┐
│    Master    │──синхронно──►│    Slave     │
│   (Primary)  │              │   (Standby)  │
└──────────────┘              └──────────────┘

При partition между Master и Slave:
- Master не может подтвердить запись без Slave
- Варианты:
  a) Блокировать записи (теряем A)    → это CP
  b) Продолжить без slave (теряем C)  → это AP

Нельзя быть CA при partition — нужно выбрать!
```

---

## 8. Критика CAP теоремы и её ограничения

### Основные претензии к CAP

```
┌─────────────────────────────────────────────────────────┐
│           Критика и ограничения CAP теоремы            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. СЛИШКОМ УПРОЩЁННАЯ МОДЕЛЬ                          │
│     ─────────────────────────                          │
│     • Бинарный выбор: либо есть, либо нет              │
│     • В реальности — спектр компромиссов               │
│     • Partition — редкое событие, а не норма           │
│                                                         │
│  2. НЕТОЧНЫЕ ОПРЕДЕЛЕНИЯ                               │
│     ──────────────────────                              │
│     • "Consistency" в CAP ≠ "Consistency" в ACID       │
│     • CAP: линеаризуемость                             │
│     • ACID: целостность транзакций                     │
│     • Это путает людей                                  │
│                                                         │
│  3. ИГНОРИРУЕТ ЛАТЕНТНОСТЬ                             │
│     ────────────────────────                            │
│     • CAP не говорит о задержках                       │
│     • Система может быть "доступна"                     │
│       но отвечать 30 секунд                            │
│                                                         │
│  4. PARTITION — НЕ БИНАРНОЕ СОБЫТИЕ                   │
│     ──────────────────────────────                      │
│     • Частичные сбои                                    │
│     • Асимметричные partition                          │
│     • Временные разрывы (flapping)                     │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Уточнения от самого Брюера (2012)

```
Eric Brewer, "CAP Twelve Years Later" (2012):

┌─────────────────────────────────────────────────────────┐
│                                                         │
│  "CAP теорема слишком упрощена. В реальных системах:   │
│                                                         │
│   1. Partition — редкость, не постоянное состояние     │
│      → Можно быть CA бОльшую часть времени            │
│      → И переключаться на CP или AP при partition     │
│                                                         │
│   2. Свойства можно настраивать per-operation          │
│      → Разные операции могут требовать разных          │
│        гарантий                                         │
│                                                         │
│   3. Система может восстанавливаться после partition  │
│      → Важна стратегия компенсации                     │
│      → Eventual consistency + reconciliation           │
│                                                         │
│   4. Latency — это тоже форма недоступности            │
│      → Если ответ приходит через 5 минут,              │
│        это не availability"                             │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Пример: настраиваемая согласованность

```python
# Cassandra: выбор уровня согласованности per-request

from cassandra import ConsistencyLevel

# Для критичных данных — строгая согласованность
def write_payment(payment):
    session.execute(
        "INSERT INTO payments ...",
        consistency_level=ConsistencyLevel.QUORUM  # CP-like
    )

# Для некритичных — высокая доступность
def write_analytics_event(event):
    session.execute(
        "INSERT INTO events ...",
        consistency_level=ConsistencyLevel.ONE  # AP-like
    )

# Одна система, разные гарантии для разных операций!
```

---

## 9. PACELC теорема

### Расширение CAP

PACELC была предложена **Daniel Abadi** (Yale) в 2012 году для устранения недостатков CAP.

```
PACELC = Partition → Availability vs Consistency
         Else      → Latency vs Consistency

┌─────────────────────────────────────────────────────────┐
│                     PACELC Теорема                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  IF Partition (P):                                      │
│     → выбираем между Availability (A) и Consistency (C)│
│                                                         │
│  ELSE (нормальная работа, E):                          │
│     → выбираем между Latency (L) и Consistency (C)     │
│                                                         │
│                                                         │
│              ┌─────────────────────┐                   │
│              │   Есть Partition?   │                   │
│              └──────────┬──────────┘                   │
│                         │                               │
│            ┌────────────┴────────────┐                 │
│            │                         │                  │
│           Да                        Нет                │
│            │                         │                  │
│            ▼                         ▼                  │
│     ┌────────────┐           ┌────────────┐           │
│     │ A vs C (P) │           │ L vs C (E) │           │
│     │            │           │            │           │
│     │ PA: Avail  │           │ EL: Low    │           │
│     │ PC: Consist│           │     latency│           │
│     │            │           │ EC: Consist│           │
│     └────────────┘           └────────────┘           │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Классификация систем по PACELC

```
┌───────────────────────────────────────────────────────────────┐
│              Классификация по PACELC                          │
├───────────────────────────────────────────────────────────────┤
│                                                               │
│  Система          При Partition    При норм. работе  Класс   │
│  ───────          ─────────────    ──────────────    ─────   │
│                                                               │
│  DynamoDB         A (доступность)  L (низкая lat.)   PA/EL   │
│  Cassandra        A (доступность)  L (низкая lat.)   PA/EL   │
│  Riak             A (доступность)  L (низкая lat.)   PA/EL   │
│                                                               │
│  MongoDB          C (согласов.)    C (согласов.)     PC/EC   │
│  HBase            C (согласов.)    C (согласов.)     PC/EC   │
│  VoltDB           C (согласов.)    C (согласов.)     PC/EC   │
│                                                               │
│  PNUTS (Yahoo)    C (согласов.)    L (низкая lat.)   PC/EL   │
│                                                               │
│  Megastore(Ggl)   A (доступность)  C (согласов.)     PA/EC   │
│                                                               │
└───────────────────────────────────────────────────────────────┘
```

### Пример: почему PACELC полезнее

```python
# Пример: MongoDB vs Cassandra в нормальной работе

# MongoDB (PC/EC): даже без partition — приоритет согласованности
# Чтение с primary = всегда актуальные данные, но выше latency
mongo_client.collection.find_one(
    {"_id": "123"},
    read_preference=ReadPreference.PRIMARY  # EC: выбор consistency
)

# Cassandra (PA/EL): даже без partition — приоритет скорости
# Чтение с ближайшего узла = быстро, но м.б. устаревшие данные
session.execute(
    "SELECT * FROM users WHERE id = ?",
    [user_id],
    consistency_level=ConsistencyLevel.ONE  # EL: выбор latency
)

# CAP не различает эти системы в нормальном режиме!
# PACELC показывает: MongoDB — EC, Cassandra — EL
```

### Latency vs Consistency trade-off

```
┌─────────────────────────────────────────────────────────┐
│         Latency vs Consistency (без partition)         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Чтобы гарантировать согласованность:                  │
│  → Нужно дождаться подтверждения от других узлов       │
│  → Это занимает время (latency)                        │
│                                                         │
│  Чтобы минимизировать latency:                         │
│  → Отвечаем сразу с локальных данных                   │
│  → Не ждём синхронизации                                │
│  → Данные могут быть устаревшими                        │
│                                                         │
│                                                         │
│  Latency                                                │
│     ▲                                                   │
│     │  ★ Strong consistency                             │
│     │      (wait for quorum)                            │
│     │                                                   │
│     │         ★ Bounded staleness                       │
│     │                                                   │
│     │              ★ Session consistency               │
│     │                                                   │
│     │                   ★ Eventual consistency         │
│     │                                                   │
│     └───────────────────────────────────────► Freshness│
│         Stale                            Fresh          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 10. Практические рекомендации по выбору

### Алгоритм выбора системы

```
┌─────────────────────────────────────────────────────────┐
│          Как выбрать подходящую систему                 │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Шаг 1: Определите критичность данных                  │
│  ───────────────────────────────────                    │
│  • Финансовые транзакции? → CP                         │
│  • Социальный контент? → AP                            │
│  • Смешанные требования? → Разные хранилища            │
│                                                         │
│  Шаг 2: Оцените допустимость потерь                    │
│  ─────────────────────────────────                      │
│  • Потеря данных недопустима? → CP + durability        │
│  • Временная рассогласованность OK? → AP               │
│  • Можно восстановить данные? → AP с компенсацией     │
│                                                         │
│  Шаг 3: Учтите географию                               │
│  ───────────────────────                                │
│  • Один регион? → CP проще реализовать                 │
│  • Multi-region? → AP обычно практичнее                │
│  • Active-active? → Нужны CRDTs или разрешение конфл. │
│                                                         │
│  Шаг 4: Оцените требования к latency                   │
│  ───────────────────────────────────                    │
│  • Real-time требования? → EL (PACELC)                 │
│  • Можно подождать? → EC допустим                       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Матрица решений

```
┌─────────────────────────────────────────────────────────────────────┐
│                     Матрица выбора системы                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  Сценарий                      Рекомендация      Примеры систем     │
│  ────────                      ────────────      ──────────────     │
│                                                                     │
│  Банковские транзакции         CP                PostgreSQL,        │
│  Бронирование билетов                            MongoDB (w:maj),  │
│  Инвентарь e-commerce                            CockroachDB       │
│                                                                     │
│  Социальные сети               AP                Cassandra,         │
│  Аналитика / логи                                DynamoDB,          │
│  Сессии пользователей                            Redis              │
│                                                                     │
│  DNS / CDN                     AP                Множественные      │
│  Кэширование                                     кэши, Consul       │
│                                                                     │
│  Конфигурация кластера         CP                etcd, ZooKeeper,  │
│  Service discovery                               Consul             │
│                                                                     │
│  Игры (multi-player)           AP + компенсация  Custom solutions,  │
│                                                  CRDTs              │
│                                                                     │
│  IoT / телеметрия              AP                InfluxDB,          │
│                                                  TimescaleDB        │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### Паттерны для сложных сценариев

```python
# Паттерн 1: Saga для распределённых транзакций

class OrderSaga:
    """
    При невозможности ACID-транзакций через несколько сервисов
    используем Saga с компенсирующими операциями
    """

    def execute(self, order):
        try:
            # Шаг 1: Резервируем товар
            self.inventory_service.reserve(order.items)

            # Шаг 2: Списываем деньги
            self.payment_service.charge(order.total)

            # Шаг 3: Создаём отправку
            self.shipping_service.create_shipment(order)

        except PaymentError:
            # Компенсация: отменяем резерв
            self.inventory_service.cancel_reservation(order.items)
            raise

        except ShippingError:
            # Компенсация: возврат денег + отмена резерва
            self.payment_service.refund(order.total)
            self.inventory_service.cancel_reservation(order.items)
            raise


# Паттерн 2: CQRS для разделения чтения и записи

class ProductService:
    """
    Записи идут в CP-систему (PostgreSQL)
    Чтения — из AP-системы (Elasticsearch)
    """

    def create_product(self, product):
        # Запись в primary (CP) — строгая согласованность
        self.postgres.insert(product)

        # Асинхронная репликация в read store (AP)
        self.event_bus.publish(ProductCreated(product))

    def search_products(self, query):
        # Чтение из read store — высокая доступность
        # Данные м.б. слегка устаревшими (eventual consistency)
        return self.elasticsearch.search(query)


# Паттерн 3: Outbox Pattern для надёжной публикации событий

class OrderRepository:
    """
    Запись данных и события в одной транзакции
    Отдельный процесс читает outbox и публикует события
    """

    def save_order(self, order):
        with self.db.transaction():
            # Сохраняем заказ
            self.db.insert('orders', order)

            # Сохраняем событие в outbox (та же транзакция!)
            event = OrderCreatedEvent(order)
            self.db.insert('outbox', {
                'event_type': 'order_created',
                'payload': event.to_json(),
                'status': 'pending'
            })

        # Outbox processor (отдельный процесс) читает и публикует
```

### Что делать при partition на практике

```
┌─────────────────────────────────────────────────────────┐
│         Практические действия при partition            │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. ДЕТЕКТИРОВАНИЕ                                      │
│     • Heartbeats между узлами                          │
│     • Timeout-based detection                          │
│     • Мониторинг (Prometheus, Grafana)                 │
│                                                         │
│  2. РЕШЕНИЕ (зависит от выбора CP/AP)                  │
│                                                         │
│     CP-система:                                         │
│     ┌───────────────────────────────────────────────┐  │
│     │ • Определить majority partition              │  │
│     │ • Minority прекращает обслуживать записи     │  │
│     │ • Клиентам minority — ошибка или redirect    │  │
│     │ • Алерт операционной команде                  │  │
│     └───────────────────────────────────────────────┘  │
│                                                         │
│     AP-система:                                         │
│     ┌───────────────────────────────────────────────┐  │
│     │ • Все узлы продолжают работать               │  │
│     │ • Записывают в локальное хранилище           │  │
│     │ • Помечают данные для последующего merge     │  │
│     │ • Предупреждают пользователей о возможной    │  │
│     │   рассогласованности                          │  │
│     └───────────────────────────────────────────────┘  │
│                                                         │
│  3. ВОССТАНОВЛЕНИЕ                                      │
│     • Синхронизация данных после восстановления сети  │
│     • Разрешение конфликтов (merge, LWW, manual)      │
│     • Проверка целостности                             │
│     • Post-mortem анализ                               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## Заключение

### Ключевые выводы

```
┌─────────────────────────────────────────────────────────┐
│                   Главные тезисы                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. CAP — это компромисс, а не ограничение             │
│     → Понимание trade-offs помогает принимать          │
│       правильные решения                                │
│                                                         │
│  2. Partition tolerance обязателен                     │
│     → В распределённых системах partition неизбежен    │
│     → Реальный выбор: CP или AP                         │
│                                                         │
│  3. Выбор можно делать per-operation                   │
│     → Одна система, разные гарантии для разных данных │
│     → Cassandra, DynamoDB позволяют настраивать        │
│                                                         │
│  4. PACELC дополняет CAP                               │
│     → Учитывает latency в нормальном режиме            │
│     → Более точно описывает поведение систем           │
│                                                         │
│  5. Нет "лучшего" выбора                               │
│     → Зависит от бизнес-требований                     │
│     → Финансы → CP, социальные сети → AP               │
│     → Часто нужны обе системы вместе                    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Дополнительные ресурсы

- **Brewer's CAP Theorem** (оригинальная статья, 2000)
- **"CAP Twelve Years Later"** — уточнения от Брюера (2012)
- **"Toward Robust Distributed Systems"** — презентация Брюера на PODC
- **"Designing Data-Intensive Applications"** — Martin Kleppmann (глава о репликации)
- **"Please stop calling databases CP or AP"** — критика упрощённого понимания CAP

---

## Шпаргалка

```
┌─────────────────────────────────────────────────────────┐
│                    CAP Cheat Sheet                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  C = Consistency (линеаризуемость)                     │
│  A = Availability (каждый запрос получает ответ)       │
│  P = Partition Tolerance (работа при сетевых сбоях)    │
│                                                         │
│  При Partition выбираем:                                │
│  • CP → откажем в обслуживании, сохраним согласованность│
│  • AP → ответим устаревшими данными, но ответим        │
│                                                         │
│  CP-системы: MongoDB, HBase, etcd, ZooKeeper           │
│  AP-системы: Cassandra, DynamoDB, CouchDB, Riak        │
│                                                         │
│  PACELC: P → A/C, Else → L/C                           │
│  (латентность тоже важна!)                              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```
